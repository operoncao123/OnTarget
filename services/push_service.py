#!/usr/bin/env python3
"""
ä¸ªæ€§åŒ–æ¨é€å¼•æ“ - æ ¹æ®ç”¨æˆ·å…³é”®è¯æ¨é€ç›¸å…³æ–‡çŒ®
"""

import json
import os
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set
from collections import defaultdict

class PersonalizedPushEngine:
    """
    ä¸ªæ€§åŒ–æ–‡çŒ®æ¨é€å¼•æ“
    - åŸºäºç”¨æˆ·å…³é”®è¯åŒ¹é…
    - å»é‡ï¼ˆç”¨æˆ·å·²çœ‹è¿‡çš„æ–‡çŒ®ï¼‰
    - ä¼˜å…ˆçº§æ’åº
    - æ¯æ—¥é™åˆ¶
    """
    
    def __init__(self, data_dir='data'):
        self.data_dir = data_dir
        self.user_papers_file = os.path.join(data_dir, 'user_papers.json')
        self.push_history_file = os.path.join(data_dir, 'push_history.json')
        
        self._ensure_data_dir()
        self.user_papers = self._load_json(self.user_papers_file)
        self.push_history = self._load_json(self.push_history_file)
    
    def _ensure_data_dir(self):
        """ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
    
    def _load_json(self, filepath: str) -> Dict:
        """åŠ è½½JSONæ–‡ä»¶"""
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def _save_json(self, filepath: str, data: Dict):
        """ä¿å­˜JSONæ–‡ä»¶"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _calculate_group_match_score(self, paper: Dict, group: Dict) -> Dict:
        """
        è®¡ç®—æ–‡çŒ®ä¸å•ä¸ªå…³é”®è¯ç»„çš„åŒ¹é…åˆ†æ•°
        
        Args:
            paper: æ–‡çŒ®æ•°æ®
            group: å…³é”®è¯ç»„æ•°æ®ï¼ŒåŒ…å« keywords, match_mode ç­‰
            
        Returns:
            {
                'score': float,  # 0-100åˆ†
                'matched_keywords': List[str],  # åŒ¹é…åˆ°çš„å…³é”®è¯
                'match_details': Dict  # è¯¦ç»†åŒ¹é…ä¿¡æ¯
            }
        """
        # æ£€æŸ¥ paper æ˜¯å¦ä¸º None
        if paper is None:
            return {'score': 0, 'matched_keywords': [], 'match_details': {}}
        
        group_keywords = group.get('keywords', [])
        match_mode = group.get('match_mode', 'any')  # 'any' æˆ– 'all'
        min_match_score = group.get('min_match_score', 0.3)
        
        if not group_keywords:
            return {'score': 0, 'matched_keywords': [], 'match_details': {}}
        
        title = (paper.get('title') or '').lower()
        abstract = (paper.get('abstract') or '').lower()
        text = title + ' ' + abstract
        
        matched_keywords = []
        total_keyword_score = 0
        
        for keyword in group_keywords:
            kw = keyword.lower()
            kw_variants = [kw, kw.replace('-', ''), kw.replace('-', ' ')]
            
            keyword_score = 0
            for variant in kw_variants:
                # ä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…çŸ­å…³é”®è¯è¯¯åŒ¹é…
                if len(variant) <= 3:
                    # çŸ­å…³é”®è¯ä½¿ç”¨å•è¯è¾¹ç•Œ
                    pattern = r'\b' + re.escape(variant) + r'\b'
                    if re.search(pattern, title):
                        keyword_score += 5
                        break
                else:
                    # é•¿å…³é”®è¯å¯ä»¥å®½æ¾åŒ¹é…
                    if variant in title:
                        keyword_score += 5
                        break
            
            for variant in kw_variants:
                if len(variant) <= 3:
                    # çŸ­å…³é”®è¯ä½¿ç”¨å•è¯è¾¹ç•Œ
                    pattern = r'\b' + re.escape(variant) + r'\b'
                    if re.search(pattern, abstract):
                        keyword_score += 2
                        break
                else:
                    # é•¿å…³é”®è¯å¯ä»¥å®½æ¾åŒ¹é…
                    if variant in abstract:
                        keyword_score += 2
                        break
            
            if keyword_score > 0:
                matched_keywords.append(keyword)
                total_keyword_score += keyword_score
        
        # æ£€æŸ¥åŒ¹é…æ¨¡å¼
        if match_mode == 'all':
            # å¿…é¡»åŒ¹é…æ‰€æœ‰å…³é”®è¯
            if len(matched_keywords) < len(group_keywords):
                return {'score': 0, 'matched_keywords': [], 'match_details': {}}
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…ä»»ä½•å…³é”®è¯ï¼Œè¿”å›0åˆ†
        if not matched_keywords:
            return {'score': 0, 'matched_keywords': [], 'match_details': {}}
        
        # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•° (0-70åˆ†)
        keyword_score = min(70, total_keyword_score * 7)
        
        # 2. å‘è¡¨æ—¶é—´ï¼ˆ0-10åˆ†ï¼‰
        time_score = 0
        try:
            pub_date = paper.get('publication_date')
            if pub_date:
                if isinstance(pub_date, str):
                    pub_date = datetime.fromisoformat(pub_date.replace('Z', '+00:00'))
                days_old = (datetime.now() - pub_date).days
                
                if days_old <= 1:
                    time_score = 10
                elif days_old <= 3:
                    time_score = 8
                elif days_old <= 7:
                    time_score = 6
                elif days_old <= 14:
                    time_score = 4
                elif days_old <= 30:
                    time_score = 2
        except:
            pass
        
        # 3. å½±å“å› å­ï¼ˆ0-10åˆ†ï¼‰
        if_score = 0
        impact_factor = paper.get('impact_factor')
        if impact_factor:
            if impact_factor >= 20:
                if_score = 10
            elif impact_factor >= 10:
                if_score = 8
            elif impact_factor >= 5:
                if_score = 6
            elif impact_factor >= 3:
                if_score = 4
            else:
                if_score = 2
        
        # 4. æ–‡çŒ®ç±»å‹ï¼ˆ0-10åˆ†ï¼‰- Researchä¼˜å…ˆ
        type_score = 5
        paper_type = paper.get('paper_type', 'research')
        if paper_type == 'research':
            type_score = 10
        elif paper_type == 'review':
            type_score = 7
        
        total_score = keyword_score + time_score + if_score + type_score
        
        return {
            'score': total_score,
            'matched_keywords': matched_keywords,
            'match_details': {
                'keyword_score': keyword_score,
                'time_score': time_score,
                'if_score': if_score,
                'type_score': type_score
            }
        }
    
    def _calculate_paper_score(self, paper: Dict, user_keywords: List[str]) -> float:
        """
        è®¡ç®—æ–‡çŒ®ä¸ç”¨æˆ·çš„åŒ¹é…åˆ†æ•°ï¼ˆå…¼å®¹æ—§ç‰ˆï¼Œä½¿ç”¨ç”¨æˆ·æ‰€æœ‰å…³é”®è¯ä½œä¸ºä¸€ä¸ªç»„ï¼‰
        """
        if not paper:
            return 0.0
        
        # åˆ›å»ºä¸´æ—¶ç»„
        temp_group = {
            'keywords': user_keywords,
            'match_mode': 'any',
            'min_match_score': 0.3
        }
        
        result = self._calculate_group_match_score(paper, temp_group)
        return result['score']
    
    def get_personalized_papers_for_group(self, user_id: str, group: Dict,
                                         available_papers: List[Dict],
                                         limit: int = 20,
                                         exclude_seen: bool = True) -> List[Dict]:
        """
        è·å–ç‰¹å®šå…³é”®è¯ç»„çš„ä¸ªæ€§åŒ–æ–‡çŒ®
        
        Args:
            user_id: ç”¨æˆ·ID
            group: å…³é”®è¯ç»„æ•°æ®
            available_papers: å¯ç”¨çš„æ–‡çŒ®åˆ—è¡¨
            limit: è¿”å›æ•°é‡é™åˆ¶
            exclude_seen: æ˜¯å¦æ’é™¤å·²çœ‹è¿‡çš„æ–‡çŒ®
        
        Returns:
            æŒ‰ä¼˜å…ˆçº§æ’åºçš„æ–‡çŒ®åˆ—è¡¨ï¼ŒåŒ…å«ç»„åŒ¹é…ä¿¡æ¯
        """
        # è·å–ç»„æ•°æ®
        group_id = group.get('id')
        
        # è·å–å·²çœ‹è¿‡çš„æ–‡çŒ®ï¼ˆä»ç»„ç‹¬ç«‹æ•°æ®ä¸­ï¼‰
        seen_papers = set()
        if exclude_seen and group_id:
            # ä»ç»„æ•°æ®æ–‡ä»¶è¯»å–
            group_data_file = os.path.join(self.data_dir, 'group_data', f"{user_id}_{group_id}.json")
            if os.path.exists(group_data_file):
                try:
                    with open(group_data_file, 'r', encoding='utf-8') as f:
                        group_data = json.load(f)
                        seen_papers = set(group_data.get('viewed_papers', []))
                except:
                    pass
        
        # è¯„åˆ†å’Œç­›é€‰
        scored_papers = []
        for paper in available_papers:
            # è·³è¿‡æ— æ•ˆçš„æ–‡çŒ®æ•°æ®
            if not paper:
                continue
            
            paper_hash = paper.get('hash') or self._get_paper_hash(paper)
            
            # è·³è¿‡å·²çœ‹è¿‡çš„
            if paper_hash in seen_papers:
                continue
            
            # è®¡ç®—ä¸è¯¥ç»„çš„åŒ¹é…åˆ†æ•°
            match_result = self._calculate_group_match_score(paper, group)
            score = match_result['score']
            
            # åªè¦æœ‰ä»»ä½•å…³é”®è¯åŒ¹é…å°±æ˜¾ç¤ºï¼ˆåˆ†æ•°>0ï¼‰
            if score >= 1:
                paper_copy = paper.copy()
                paper_copy['personalized_score'] = score
                paper_copy['hash'] = paper_hash
                
                # æ·»åŠ ç»„åŒ¹é…ä¿¡æ¯
                paper_copy['matched_group'] = {
                    'id': group.get('id'),
                    'name': group.get('name'),
                    'icon': group.get('icon', 'ğŸ”¬'),
                    'color': group.get('color', '#5a9a8f'),
                    'match_score': score,
                    'matched_keywords': match_result['matched_keywords'],
                    'match_details': match_result['match_details']
                }
                
                scored_papers.append(paper_copy)
        
        # æŒ‰åˆ†æ•°æ’åº
        scored_papers.sort(key=lambda x: x['personalized_score'], reverse=True)
        
        # é™åˆ¶æ•°é‡
        return scored_papers[:limit]
    
    def get_personalized_papers(self, user_id: str, user_keywords: List[str], 
                               available_papers: List[Dict], 
                               limit: int = 20,
                               exclude_seen: bool = True) -> List[Dict]:
        """
        è·å–ä¸ªæ€§åŒ–æ¨é€çš„æ–‡çŒ®
        
        Args:
            user_id: ç”¨æˆ·ID
            user_keywords: ç”¨æˆ·çš„å…³é”®è¯
            available_papers: å¯ç”¨çš„æ–‡çŒ®åˆ—è¡¨
            limit: è¿”å›æ•°é‡é™åˆ¶
            exclude_seen: æ˜¯å¦æ’é™¤å·²çœ‹è¿‡çš„æ–‡çŒ®
        
        Returns:
            æŒ‰ä¼˜å…ˆçº§æ’åºçš„æ–‡çŒ®åˆ—è¡¨
        """
        # è·å–ç”¨æˆ·å·²çœ‹è¿‡çš„æ–‡çŒ®
        seen_papers = set()
        if exclude_seen and user_id in self.user_papers:
            seen_papers = set(self.user_papers[user_id].get('seen_papers', []))
        
        # è¯„åˆ†å’Œç­›é€‰
        scored_papers = []
        for paper in available_papers:
            # è·³è¿‡æ— æ•ˆçš„æ–‡çŒ®æ•°æ®
            if not paper:
                continue
            
            paper_hash = paper.get('hash') or self._get_paper_hash(paper)
            
            # è·³è¿‡å·²çœ‹è¿‡çš„
            if paper_hash in seen_papers:
                continue
            
            # è®¡ç®—åˆ†æ•°
            score = self._calculate_paper_score(paper, user_keywords)
            
            # åªè¦æœ‰ä»»ä½•å…³é”®è¯åŒ¹é…å°±æ˜¾ç¤ºï¼ˆåˆ†æ•°>0ï¼‰
            if score >= 1:
                paper_copy = paper.copy()
                paper_copy['personalized_score'] = score
                paper_copy['hash'] = paper_hash
                scored_papers.append(paper_copy)
        
        # æŒ‰åˆ†æ•°æ’åº
        scored_papers.sort(key=lambda x: x['personalized_score'], reverse=True)
        
        # é™åˆ¶æ•°é‡
        return scored_papers[:limit]
    
    def _get_paper_hash(self, paper: Dict) -> str:
        """ç”Ÿæˆæ–‡çŒ®å“ˆå¸Œ"""
        import hashlib
        doi = paper.get('doi', '')
        pmid = paper.get('pmid', '')
        title = paper.get('title', '').lower().strip()
        
        if doi:
            return hashlib.md5(f"doi:{doi}".encode()).hexdigest()
        elif pmid:
            return hashlib.md5(f"pmid:{pmid}".encode()).hexdigest()
        else:
            return hashlib.md5(f"title:{title}".encode()).hexdigest()
    
    def mark_papers_as_seen(self, user_id: str, paper_hashes: List[str]):
        """
        æ ‡è®°æ–‡çŒ®ä¸ºå·²çœ‹è¿‡
        
        Args:
            user_id: ç”¨æˆ·ID
            paper_hashes: æ–‡çŒ®å“ˆå¸Œåˆ—è¡¨
        """
        if user_id not in self.user_papers:
            self.user_papers[user_id] = {
                'seen_papers': [],
                'saved_papers': [],
                'interactions': []
            }
        
        # æ·»åŠ æ–°çš„å·²çœ‹æ–‡çŒ®
        for paper_hash in paper_hashes:
            if paper_hash not in self.user_papers[user_id]['seen_papers']:
                self.user_papers[user_id]['seen_papers'].append(paper_hash)
        
        self._save_json(self.user_papers_file, self.user_papers)
    
    def save_paper_for_user(self, user_id: str, paper_hash: str):
        """
        ç”¨æˆ·æ”¶è—æ–‡çŒ®
        
        Args:
            user_id: ç”¨æˆ·ID
            paper_hash: æ–‡çŒ®å“ˆå¸Œ
        """
        if user_id not in self.user_papers:
            self.user_papers[user_id] = {
                'seen_papers': [],
                'saved_papers': [],
                'interactions': []
            }
        
        if paper_hash not in self.user_papers[user_id]['saved_papers']:
            self.user_papers[user_id]['saved_papers'].append(paper_hash)
            self._save_json(self.user_papers_file, self.user_papers)
    
    def unsave_paper_for_user(self, user_id: str, paper_hash: str):
        """å–æ¶ˆæ”¶è—"""
        if user_id in self.user_papers:
            if paper_hash in self.user_papers[user_id]['saved_papers']:
                self.user_papers[user_id]['saved_papers'].remove(paper_hash)
                self._save_json(self.user_papers_file, self.user_papers)
    
    def record_interaction(self, user_id: str, paper_hash: str, 
                          interaction_type: str, metadata: Dict = None):
        """
        è®°å½•ç”¨æˆ·äº¤äº’
        
        Args:
            user_id: ç”¨æˆ·ID
            paper_hash: æ–‡çŒ®å“ˆå¸Œ
            interaction_type: äº¤äº’ç±»å‹ï¼ˆview, click, save, shareï¼‰
            metadata: é¢å¤–å…ƒæ•°æ®
        """
        if user_id not in self.user_papers:
            self.user_papers[user_id] = {
                'seen_papers': [],
                'saved_papers': [],
                'interactions': []
            }
        
        interaction = {
            'paper_hash': paper_hash,
            'type': interaction_type,
            'timestamp': datetime.now().isoformat(),
            'metadata': metadata or {}
        }
        
        self.user_papers[user_id]['interactions'].append(interaction)
        
        # é™åˆ¶äº¤äº’è®°å½•æ•°é‡ï¼ˆä¿ç•™æœ€è¿‘1000æ¡ï¼‰
        interactions = self.user_papers[user_id]['interactions']
        if len(interactions) > 1000:
            self.user_papers[user_id]['interactions'] = interactions[-1000:]
        
        self._save_json(self.user_papers_file, self.user_papers)
    
    def get_user_feed(self, user_id: str, user_keywords: List[str],
                     all_papers: List[Dict], page: int = 1, 
                     per_page: int = 10) -> Dict:
        """
        è·å–ç”¨æˆ·çš„ä¸ªæ€§åŒ–æ–‡çŒ®æµ
        
        Args:
            user_id: ç”¨æˆ·ID
            user_keywords: ç”¨æˆ·å…³é”®è¯
            all_papers: æ‰€æœ‰å¯ç”¨æ–‡çŒ®
            page: é¡µç 
            per_page: æ¯é¡µæ•°é‡
        
        Returns:
            åŒ…å«æ–‡çŒ®åˆ—è¡¨å’Œåˆ†é¡µä¿¡æ¯çš„å­—å…¸
        """
        # è·å–ä¸ªæ€§åŒ–æ’åºçš„æ–‡çŒ®
        personalized = self.get_personalized_papers(
            user_id, user_keywords, all_papers, 
            limit=page * per_page
        )
        
        # åˆ†é¡µ
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        page_papers = personalized[start_idx:end_idx]
        
        # è·å–ç”¨æˆ·æ”¶è—çš„æ–‡çŒ®
        saved_papers = []
        if user_id in self.user_papers:
            saved_hashes = set(self.user_papers[user_id].get('saved_papers', []))
            for paper in all_papers:
                paper_hash = paper.get('hash') or self._get_paper_hash(paper)
                if paper_hash in saved_hashes:
                    saved_papers.append(paper)
        
        return {
            'papers': page_papers,
            'saved_papers': saved_papers,
            'total_available': len(personalized),
            'page': page,
            'per_page': per_page,
            'has_more': len(personalized) > end_idx
        }
    
    def get_user_stats(self, user_id: str) -> Dict:
        """è·å–ç”¨æˆ·çš„é˜…è¯»ç»Ÿè®¡"""
        if user_id not in self.user_papers:
            return {
                'total_seen': 0,
                'total_saved': 0,
                'interactions_7d': 0,
                'interactions_30d': 0,
                'favorite_keywords': []
            }
        
        user_data = self.user_papers[user_id]
        interactions = user_data.get('interactions', [])
        
        # è®¡ç®—æœ€è¿‘7å¤©å’Œ30å¤©çš„äº¤äº’æ•°
        now = datetime.now()
        interactions_7d = 0
        interactions_30d = 0
        
        for interaction in interactions:
            try:
                ts = datetime.fromisoformat(interaction['timestamp'].replace('Z', '+00:00'))
                days_diff = (now - ts).days
                if days_diff <= 7:
                    interactions_7d += 1
                if days_diff <= 30:
                    interactions_30d += 1
            except:
                continue
        
        return {
            'total_seen': len(user_data.get('seen_papers', [])),
            'total_saved': len(user_data.get('saved_papers', [])),
            'interactions_7d': interactions_7d,
            'interactions_30d': interactions_30d,
            'favorite_keywords': self._extract_favorite_keywords(interactions)
        }
    
    def _extract_favorite_keywords(self, interactions: List[Dict]) -> List[str]:
        """ä»äº¤äº’ä¸­æå–ç”¨æˆ·æœ€æ„Ÿå…´è¶£çš„å…³é”®è¯"""
        # ç®€åŒ–çš„å®ç°ï¼šåŸºäºäº¤äº’é¢‘ç‡
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPåˆ†æ
        keyword_count = defaultdict(int)
        
        for interaction in interactions:
            metadata = interaction.get('metadata', {})
            keywords = metadata.get('keywords', [])
            for kw in keywords:
                keyword_count[kw.lower()] += 1
        
        # è¿”å›å‰5ä¸ªå…³é”®è¯
        sorted_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)
        return [kw for kw, count in sorted_keywords[:5]]
    
    def get_push_history(self, user_id: str = None, days: int = 7) -> List[Dict]:
        """
        è·å–æ¨é€å†å²
        
        Args:
            user_id: ç‰¹å®šç”¨æˆ·ï¼ˆNoneè¡¨ç¤ºæ‰€æœ‰ç”¨æˆ·ï¼‰
            days: æœ€è¿‘å¤šå°‘å¤©
        """
        cutoff = (datetime.now() - timedelta(days=days)).isoformat()
        
        history = []
        for push_id, push_data in self.push_history.items():
            if push_data['timestamp'] >= cutoff:
                if user_id is None or push_data.get('user_id') == user_id:
                    history.append(push_data)
        
        history.sort(key=lambda x: x['timestamp'], reverse=True)
        return history
    
    def record_push(self, user_id: str, paper_hashes: List[str], 
                   push_type: str = 'daily'):
        """
        è®°å½•ä¸€æ¬¡æ¨é€
        
        Args:
            user_id: ç”¨æˆ·ID
            paper_hashes: æ¨é€çš„æ–‡çŒ®å“ˆå¸Œåˆ—è¡¨
            push_type: æ¨é€ç±»å‹ï¼ˆdaily, weekly, instantï¼‰
        """
        push_id = f"{user_id}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        self.push_history[push_id] = {
            'id': push_id,
            'user_id': user_id,
            'paper_hashes': paper_hashes,
            'timestamp': datetime.now().isoformat(),
            'type': push_type,
            'count': len(paper_hashes)
        }
        
        self._save_json(self.push_history_file, self.push_history)
    
    def cleanup_old_data(self, days: int = 90):
        """
        æ¸…ç†æ—§æ•°æ®
        
        Args:
            days: æ¸…ç†è¶…è¿‡å¤šå°‘å¤©çš„æ•°æ®
        """
        cutoff = (datetime.now() - timedelta(days=days)).isoformat()
        
        # æ¸…ç†æ¨é€å†å²
        pushes_to_remove = []
        for push_id, push_data in self.push_history.items():
            if push_data['timestamp'] < cutoff:
                pushes_to_remove.append(push_id)
        
        for push_id in pushes_to_remove:
            del self.push_history[push_id]
        
        # æ¸…ç†ç”¨æˆ·äº¤äº’è®°å½•
        for user_id, user_data in self.user_papers.items():
            interactions = user_data.get('interactions', [])
            filtered_interactions = [
                i for i in interactions 
                if i.get('timestamp', '2000-01-01') >= cutoff
            ]
            user_data['interactions'] = filtered_interactions
        
        self._save_json(self.push_history_file, self.push_history)
        self._save_json(self.user_papers_file, self.user_papers)
        
        return {
            'removed_pushes': len(pushes_to_remove)
        }


class PushScheduler:
    """
    æ¨é€è°ƒåº¦å™¨
    ç®¡ç†å®šæ—¶æ¨é€ä»»åŠ¡
    """
    
    def __init__(self, push_engine: PersonalizedPushEngine):
        self.push_engine = push_engine
    
    def schedule_daily_push(self, user_manager, paper_cache, 
                           send_callback=None):
        """
        æ‰§è¡Œæ¯æ—¥æ¨é€
        
        Args:
            user_manager: ç”¨æˆ·ç®¡ç†å™¨å®ä¾‹
            paper_cache: æ–‡çŒ®ç¼“å­˜å®ä¾‹
            send_callback: å‘é€æ¨é€çš„å›è°ƒå‡½æ•°
        """
        users = user_manager.get_all_users()
        push_results = []
        
        for user_id, user_info in users.items():
            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å¯ç”¨æ¨é€
            if not user_info.get('is_active', True):
                continue
            
            # è·å–ç”¨æˆ·å®Œæ•´ä¿¡æ¯
            user_full = user_manager.users.get(user_id)
            if not user_full:
                continue
            
            preferences = user_full.get('preferences', {})
            if not preferences.get('email_notifications', True):
                continue
            
            # è·å–ç”¨æˆ·å…³é”®è¯
            keywords = user_full.get('keywords', [])
            if not keywords:
                continue
            
            # ä»ç¼“å­˜è·å–åŒ¹é…çš„å…³é”®è¯æ–‡çŒ®
            from smart_cache import SmartCache
            cache = SmartCache()
            paper_hashes = cache.find_papers_by_keywords(keywords)
            papers = cache.batch_get_papers(paper_hashes)
            
            # è·å–ä¸ªæ€§åŒ–æ¨é€åˆ—è¡¨
            daily_limit = preferences.get('daily_limit', 20)
            personalized = self.push_engine.get_personalized_papers(
                user_id, keywords, papers, limit=daily_limit
            )
            
            if personalized:
                # è®°å½•æ¨é€
                paper_hashes = [p['hash'] for p in personalized]
                self.push_engine.record_push(user_id, paper_hashes, 'daily')
                
                # è°ƒç”¨å‘é€å›è°ƒ
                if send_callback:
                    send_callback(user_id, user_info, personalized)
                
                push_results.append({
                    'user_id': user_id,
                    'username': user_info['username'],
                    'papers_count': len(personalized)
                })
        
        return push_results
